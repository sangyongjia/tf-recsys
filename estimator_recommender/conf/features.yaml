## Feature Configuration
# Using tf.feature_column in TensorFlow.
# see https://www.tensorflow.org/api_docs/python/tf/feature_column

# Each feature consists 3 attributes `type`, `transform`, `parameter`.
# 1. feature: feature name required, must in schema.yaml.
# 2. type: required, feature type, `category` or `continuous`.
# 3. transform: feature transform.
# 4. parameter: main parameter for transform.
#    (1) type: category
#         transform: `hash_bucket` or `vocab` or `identity`.
#           hash_bucket  ==> tf.feature.categorical_column_with_hash_bucket
#           vocab        ==> tf.feature.categorical_column_with_vocabulary_list
#           identity     ==> tf. feature.categorical_column_with_identity
#         parameter: examples as follows,
#           1000            (hash_bucket_size  for `hash_bucket`)
#           ['a', 'b', 'c'] (vocabulary_list for `vocab`)
#           15              (num_buckets  for `identity`)
#    (2) type: continuous
#         transform: ``, `log`, `standard` normalization for normalizer_fn in
#                    tf.feature_column.numeric_column, set empty to not do normalization.
#           ``    ==> x = (x-min) / (x-max);
#           `log`        ==> x = log(x), all feature values must >= 1
#           `standard`   ==> x = (x-mean) / std
#
#         parameter:
#           normalization: [min, max] or [mean, std] list for `` or `standard`; set empty for `log`.
#           boundaries: optional, set boundaries, eg: [5, 10, 15, 20] for `discretize`
#                       (bucketized continuous feature for wide input or as cross feature),
#                       set empty for not use continuous feature for wide input.
# Set unused features by using symbol `#` ahead of the lines.
# Category features with hash_bucket using embedding_column to feed deep, others by indicator_column.
# All listed features are used in model.

# Q & A about hash_bucket_size:
# If category size=1000, how much should hash_bucket_size be ?
#   An interesting discovery is that randomly chose N number a_i between 1~N, i=1,...N
#     let b_i = a_i % N, the distinct b_i from all N number is about 0.633.
#     in other words, a random hash func chose N as hash_bucket_size collision rate is 0.633.
#   Recommend `hash_bucket_size` to be 2~3*category size.
#     larger `hash_bucket_size` require more memory and complexity, but smaller cause more collision
#   Here use the strategy that
#     for low sparsity category, set `hash_bucket_size` 3~4*category size to reduce collision
#     for high sparsity category, set 1.5~2*category size to save memory.
# TODO: support all tf.feature_column.

#R1:
#  type: continuous
#  transform:
#  parameter:
#    normalization:
#    boundaries:
#['userid','item','category','buy_flag']
userid:
  type: category
  transform: hash_bucket
  parameter:
    hash_bucket_size: 60000
    embed_size: 4


item:
  type: category
  transform: hash_bucket
  parameter:
    hash_bucket_size: 1500000
    embed_size: 4


category:
  type: category
  transform: hash_bucket
  parameter:
    hash_bucket_size: 14000
    embed_size: 4